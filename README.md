# 4650-implicit-hate_study
Transformer-based  models  are  widely  used  and very successful in a variety of language-based tasks. However, while their complicated structure allows for state-of-the-art results, it also poses a challengein the space of explainability.  Hate speech detection is one critical domain where explainability is crucial as hate speech becomes more prominent in online communities. In this project, we interpret SVM and BERT models trained on an implicit hate-speech dataset introduced by El Sherief et al. using state-of-the-art explainability frameworks for machine learning models, first and foremost being SHAP (Lundberg et al.  2017).  After achieving comparable classification results on both SVM and BERT to previous models built on this dataset, we conducted preliminary analysis and found that binary models are putting the most focus on seemingly neutral words instead of known hate lexicon, while the multi-class models are behaving more as expected with an emphasis on known hate lexicons.  We plan toconduct further analysis on this pattern using addi-tional strategies to SHAP and include an analysisof ELECTRA.

