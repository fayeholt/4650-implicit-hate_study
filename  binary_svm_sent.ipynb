{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "binary-svm-sent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fayeholt/4650-implicit-hate_study/blob/main/%20binary_svm_sent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdXXHuYgjMUc",
        "outputId": "96ba02e4-0277-4573-cb71-ff59665b7e69"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection, naive_bayes, svm\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "import nltk\n",
        "import torch\n",
        "# import shap\n",
        "!pip install shap\n",
        "import shap\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "clf = svm.SVC(kernel='linear')\n",
        "\n",
        "name = 'hydrated_hate.csv'\n",
        "dataset = pd.read_csv(name)\n",
        "\n",
        "dataset = dataset[dataset.content != \"NOT AVAILABLE\"]\n",
        "# dataset = dataset[dataset['class'] != 'not_hate']\n",
        "dataset.drop('ID', inplace=True, axis=1)\n",
        "dataset.content = dataset.content.astype(str)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shap in /usr/local/lib/python3.7/dist-packages (0.40.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from shap) (0.22.2.post1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.7/dist-packages (from shap) (4.62.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from shap) (1.1.5)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.7/dist-packages (from shap) (21.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from shap) (0.51.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from shap) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from shap) (1.19.5)\n",
            "Requirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.7/dist-packages (from shap) (0.0.7)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>20.9->shap) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->shap) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->shap) (0.34.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->shap) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->shap) (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0lHjXyEHKC6"
      },
      "source": [
        "for index, entry in dataset.iterrows():\n",
        "    if entry['class'] == 'not_hate':\n",
        "      dataset.loc[index, 'label'] = 0\n",
        "    else:\n",
        "      dataset.loc[index, 'label'] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adsc2jycE757"
      },
      "source": [
        "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(dataset['content'], dataset['label'], test_size=0.3)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "\n",
        "# Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
        "# Tfidf_vect.fit(dataset['content'])\n",
        "# Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
        "# Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
        "\n",
        "vectorizer = TfidfVectorizer(min_df = 5,\n",
        "                             max_df = 0.8,\n",
        "                             sublinear_tf = True,\n",
        "                             use_idf = True)\n",
        "train_vectors = vectorizer.fit_transform(Train_X)\n",
        "test_vectors = vectorizer.transform(Test_X)\n",
        "vectorizer.vocabulary_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3OXdJYP_o0t"
      },
      "source": [
        "shap.initjs()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ2gJpiWbkh7"
      },
      "source": [
        "import time\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report\n",
        "# Perform classification with SVM, kernel=linear\n",
        "classifier_linear = svm.SVC(kernel='linear', probability=True)\n",
        "t0 = time.time()\n",
        "classifier_linear.fit(train_vectors, Train_Y)\n",
        "t1 = time.time()\n",
        "prediction_linear = classifier_linear.predict(test_vectors)\n",
        "t2 = time.time()\n",
        "time_linear_train = t1-t0\n",
        "time_linear_predict = t2-t1\n",
        "# results\n",
        "print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n",
        "report = classification_report(Test_Y, prediction_linear, output_dict=True)\n",
        "print(report)\n",
        "# print('negative: ', report['neg'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaohcjKQFsV3"
      },
      "source": [
        "svm_explainer = shap.KernelExplainer(classifier_linear.predict_proba,train_vectors)\n",
        "svm_shap_values = svm_explainer.shap_values(X=test_vectors[:10], nsamples=10, l1_reg=\"num_features(4)\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwSEfoXv7oUr"
      },
      "source": [
        "shap.summary_plot(svm_shap_values, test_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-DYTjwY_BA7"
      },
      "source": [
        "shap.force_plot(svm_explainer.expected_value[0], svm_shap_values[0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}